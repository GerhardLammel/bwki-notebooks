{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge!!!\n",
    "\n",
    "Du hast in den vorherigen Aufgaben viele Methoden des maschinellen Lernens kennen gelernt. In dieser Challenge kannst du zeigen, was du drauf hast! Entwickle ein neuronales Netzwerk, dass die Hunde als krank oder gesund diagnostiziert. In der Bestenliste kannst du sehen\\\n",
    ", wie gut dein Netzwerk im Vergleich zu den anderen Teilnehmern abschneidet. Kannst du eine bessere Genauigkeit erzielen? Eure Anstrengung wird sich lohnen: Die besten Teams ziehen - unabhängig von den erreichten Punkten in den Aufgaben oder die vorgeschlagene Idee - in\\\n",
    " die zweite Runde ein!\n",
    "\n",
    "Die Bestenliste findest du auf der Startseite mit den verschiedenen Aufgabentypen in der oberen Zeile.\n",
    "\n",
    "Am Besten fängst du mit einem einfachen Netzwerk an und veränderst es. Dabei solltest du immer auch Over- und Underfitting im Auge\n",
    "behalten. Du hast viele Möglichkeiten das Netzwerk oder die Optimierung anzupassen:\n",
    "\n",
    " - Anzahl der Layer\n",
    " - Anzahl der Neuronen pro Layer\n",
    " - Regularisierung (Dropout, weight decay)\n",
    " - Unterschiedlieche Optimierer (Adam, SGD, ...)\n",
    " - Learning rate decay\n",
    " - Early stopping\n",
    " - Andere non-linearities\n",
    " - Normalisierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Daten laden\n",
    "D = np.load('data/challenge_D_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 2000\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 20\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            out = out*np.nan\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
